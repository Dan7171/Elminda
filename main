import array
from itertools import groupby
from pathlib import Path
from time import strftime
from matplotlib import pyplot as plt, test
from pyparsing import trace_parse_action

import sklearn
from sklearn.feature_selection import SelectKBest, f_classif, f_regression
from sklearn.feature_selection import chi2
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
import pandas as pd
import numpy as np
import datetime as dt
import seaborn as sns
import warnings
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
warnings.simplefilter("ignore", UserWarning)
#import the_module_that_warns


def reformat_dates(df): 
    df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)
    df['just_date'] = pd.to_datetime(df['date']).dt.date
    newdate = dt.date(1998, 11, 8)
    df['just_date'].replace({pd.NaT: newdate}, inplace=True)
    df['just_date'] = df['just_date'].apply(lambda d: d.strftime("%d/%m/%Y"))
    addcol = df.pop('just_date')
    df.insert(2, 'just_date', addcol)
 

def unite_same_day_visits_to_the_first(df):
    """ for a subject with more than one EEG recording in the same day, keep only the first visit"""
    df = df.sort_values(by=['subject','date']) #sorting visits by subject, and then by date
    df = df.reset_index()
    df = df.drop_duplicates(subset=['subject', 'just_date'], keep='first') #removes rows(visits) who has same subject with the same date, but leaves the first appearance
    df = df.reset_index()


def final_date_merge(df1, df2): 
    """merging df1 and df2 by subject and creates a final_date column for each visit """
    t = 2 #t is the valid maximun difference in days
    df = df1.merge(df2, how = 'inner',on = ['subject']) #leaving in the df all pairs with the same subject
    # Considering only visits where the bna date of visit and the clinical date of visit have at most
    # t days between them, and uniting them with one 'final_date'.Note that the function is droping duplicates and keeping
    # only the first appearance of a final date, in order to prevent duplication of 'fianl_date's of visits.
    #we want each visit (a pair of clinical visit and bna visit) to have one final_date
    df['final_date'] =[d1 if time_difference(d1,d2)<= t else "remove" for d1,d2 in zip(df['just_date_x'],df['just_date_y'])] 
    df = df[df['final_date']!= 'remove']
    df = df.reset_index()
    df = df.drop_duplicates(subset=['final_date','subject'], keep='first')
    df = df.reset_index()
    return df 
 
def time_difference(d1_str, d2_str): #v
    d1 = dt.datetime.strptime(d1_str, "%d/%m/%Y")
    d2 = dt.datetime.strptime(d2_str, "%d/%m/%Y")
    delta = d2-d1
    difference = abs(delta.days)
    return difference   

def save_df_to_csv(name, df):
    path = Path()
    df.to_csv((name + '.csv'),index = False)


def get_percantage_change(df, column_name):
    pass
     

def group_by(df,c): #visits.groupby(['subject']), #df.groupby([c])
    """ equivalent to df.groupby([c]), I wrote this func for exampling the outputs!

    given a df (visits) returns all of the sub dfs, where each sub df belongs so one subjects:
    sub44
        visit  final_date
    46      1  22/08/2016
    47      2  29/08/2016
    48      3  09/05/2016
    49      5  20/09/2016
    50      6  27/09/2016
    sub83
        visit  final_date  HDRS-17
    53      1  12/11/2016     23.0
    54      2  18/12/2016     22.0
    55      3  25/12/2016     21.0
    56      4  01/01/2017     21.0
    57      5  01/08/2017     20.0
    58      6  16/01/2017     14.0
    .
    .
    ."""
    return df.groupby([c])
    
 

def get_group_by_name_from_all_groups(groups_by_coulumn,desired_group_name):
    """first getting all athe groups by 'subject'picks particulary the group of the subject passed in subject_id
    example: input = (groups_by_coulumn=visits.groupby['subject] (all groups by subjects),desired_group_name='sub83') -> output = :
    
    output = sub83,
    sub83
        visit  final_date  HDRS-17 .....
    53      1  12/11/2016     23.0
    54      2  18/12/2016     22.0
    55      3  25/12/2016     21.0
    56      4  01/01/2017     21.0
    57      5  01/08/2017     20.0
    58      6  16/01/2017     14.0"""
    
    for name, group in groups_by_coulumn: #name is group name('subject'), group is all its df
        if name == desired_group_name:
            return group # a df
            #print(name)
            #print(group[['visit','final_date']]) #see output example in ducumentation

def get_subject_first_visit_val_in_column_c(subject_id,d,c,):
    group = d[subject_id] # O(1)
    #group.sort_values(by='visit')
    return group.iloc[0][c]
    
def get_subject_last_visit_val_in_column_c(subject_id,d,c):
    group = d[subject_id] # O(1)
    #group.sort_values(by='visit')
    return group.iloc[len(group)-1][c]

def print_interesting_values_by_subject_grouping(df,d):
    """adding desired priniting columns which are unneccessary in model (like first_visit_HDRS-17),
    printing frouping by subject of all wanted featured and then deleting the added unnccessary ones"""
    add_HDRS_first_last_columns(visits,d) #adding 'first_visit_HDRS-17','last_visit_HDRS-17'  columns to visits
    #after addign unnccessary columns: print groups by subject:
    for name,group in group_by(df,'subject'):
        print(name)
        print(group[['visit','final_date','HDRS-17']])
        print(group[['first_visit_HDRS-17','last_visit_HDRS-17','change_HDRS-17']])
    #remove unneccessary
    df.drop(columns=['first_visit_HDRS-17','last_visit_HDRS-17'])



def get_subjects_with_increase_in_HDRS_or_with_zero_start_HDRS(visits):
    """returning all the strange subjects that has HDRS-17 == 0 in the
    first visit or had a positive percantage change in change_HDRS-17 (increased depression)"""
    
    strange_subjects = set(visits.iloc[i]['subject'] for i in range(len(visits)) if (visits.iloc[i]['change_HDRS-17'] == None or float(visits.iloc[i]['change_HDRS-17']) > 0))
    return strange_subjects
    
def get_subject_change_rate_in_column_c(subject,d,c):
    """Given a subject (value in the column 'subject') and a dictionary of key= 'subject' val = this 'subject' df,
    returns the value of the change rate from before to after, in the column named c,
    where the before change is the value of c in the first visit of this subject and
    after change is the value of c in the last visit of this subject."""
    subject_df = d[subject]
    before_change_val = subject_df.iloc[0][c] #the value of the 
    after_change_val = subject_df.iloc[len(subject_df)-1][c]
    if before_change_val != 0:
        percantage_change = ((after_change_val - before_change_val) / before_change_val) * 100 
    else:
        percantage_change =  None
    return percantage_change

def add_HDRS_first_last_columns(df,d):
    """adding the mentioned colums columns which er are interested at about HDRS-17 to the df"""
    df['first_visit_HDRS-17'] = df['subject'].apply(lambda subject: get_subject_first_visit_val_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))
    df['last_visit_HDRS-17'] = df['subject'].apply(lambda subject: get_subject_last_visit_val_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))


def select_features_by_univariate_selection(df): 
    "by chi square, anova test, or corellation coefficient(pearson or spirman)?"
    "we will use the method selectKbest- which finds the K best features in terms of maximun corellation with the y variant"    
    pass
def seletct_features_by_feature_importance(df): #all 3 technices are usefull for small data sets only. not usefull for us.
    "part of the wraper methods"
    "USEFUL ONLY IN SMALL DATA SETS"
    "by wraper method. Some of them with no statistical tasks, but only one of 3 mechanismes:"
    "forward selection/backward elimination/recursion feature elimination. forward selection- start with no features, adding only highly corellated with y features to the model.  backward elimination: start with all of features, find the least significant feature on y, if its p val >0.05 drop it else leave it,"
    "and move on to the next feature untill end of features"
    pass
def select_features_by_correlation_matrix_heat_map(df):
    pass
def change_gender_column_to_numeric_binary(df,gender_col_name):
    """convert gender: 'male'/'female' -> 0/1 (so we can use it numericaly) under the couln named gender_col_name"""

    df[gender_col_name] = df[gender_col_name].apply(lambda x: 1 if x == "Male" else 0)
    for col in df.columns:
        if col != "subject":
            df[col] = pd.to_numeric(df[col], errors='coerce')
    return df
def keep_first_visit_only(df):
    df.sort_values(by='visit')
    df = df.groupby('subject', as_index=False).first() #keeps only the first visit of each subject   
    return df

def drop_cols(df,cols_to_drop:set):
    """Given a set 'cols_to_drop' of column names, removing the columns in it from the df and returns it"""
    
    df = df[[c for c in df.columns if c not in cols_to_drop]] # allowing only columns we weren't marked to remove
    return df
def get_set_without_items(s:set,items:list):
    """Given a set s and a list of items in it, returns a new set without those items"""
    for item in items:
        if item in s:
            s.remove(item)
    return s

def print_corr_mat(df):
    shapes = (20,20)
    plt.figure(figsize=shapes)
    corr = df.corr()
    sns.heatmap(corr,annot=True,cmap=plt.cm.CMRmap_r)
    plt.show()


#*** MAIN PART : ***

#PART A: Prepare data for work:
bna = pd.read_csv('BNA_data.csv') 
clinical = pd.read_csv("Clinical_data_hebrew.csv") #Hebrew is reversed in the dataframe
reformat_dates(bna)  
reformat_dates(clinical)
unite_same_day_visits_to_the_first(bna)  
#merge theh two data frames (bna and clinical) by 'subject' and 'final_date' of visit
visits = final_date_merge(bna, clinical) 

#create a dictionary of key = 'subject' to val = all the df values belong to it (for time complexity reasons):
visits_grouped_by_subject = group_by(visits,'subject') # all groups by subject, same as df.groupby(['subject])
subject_to_subject_group = {}
for name,group in visits_grouped_by_subject: # map all groups by key= name= subject, val = group = subject's df (for time complexity reasons)
    group.sort_values(by='visit')
    subject_to_subject_group[name] = group

#PART B: pick a model: (we picked linear regression)

#PART C: pick a 'y' to predict and add it to the df
# preparing linear regression 'y' vector: ('change_HDRS-17') 
visits['change_HDRS-17'] = visits['subject'].apply(lambda subject: get_subject_change_rate_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))

#PART D: generate 'X' vector, by dropping out columns and rows we can't or don't want to rely on when predicting:

# preparing linear regression 'X' vector:
visits_linear_reg = visits

#step 1: keep first visit of each subject and only it (as 'baseline' 'X' values)
visits_linear_reg = keep_first_visit_only(visits_linear_reg) 


#step 2: change 'gender' from 'male','female' to 1,0 (linear regression demands numeric values):
visits_linear_reg = change_gender_column_to_numeric_binary(visits_linear_reg,'gender')# turning 'female' to 0 and 'male' to 1 under 'gender' column:

#step 3: remove non numerical and 'noisy' columns (like dates, visit numbers etc..) from the df:
#prepare one big set of them:
#3.1 pick the 'noisy' and non numerical columns:
print("before step 3, shape =", visits_linear_reg.shape)
clinical_col_set = set(list(clinical.columns.values))

allowed_cols_clinical = ['Weight in Kg','height in cm ','BMI','Smoking?']
restricted_cols_clinical = get_set_without_items(clinical_col_set, items = allowed_cols_clinical)

restricted_cols_bna = set(['taskData.elm_id','EEG NUMBER','visit','date','ageV1'])
restricted_cols_visits =set(['subject', 'level_0','index','date_x','date_y','just_date_x','just_date_y','final_date',])
cols_to_drop = restricted_cols_visits.union(restricted_cols_bna.union(restricted_cols_clinical)) #the set
#3.2: drop them:
visits_linear_reg = drop_cols(visits_linear_reg, cols_to_drop) 
print("after step 3, shape =", visits_linear_reg.shape)
# parts E-H: splitting, feature selection, training, predicting and testing

X = visits_linear_reg.drop(columns= ['change_HDRS-17'])
y = visits_linear_reg['change_HDRS-17']
X.fillna(0, inplace = True)
y.fillna(0, inplace = True)

 
#PART E: select k best features from the 'X' vector, by the 'MRMR' (maximun relevancy, minimun redundancy) principal:
#for the actual machine learning model:

select = SelectKBest(score_func=f_regression,k=40)
X_new = select.fit_transform(X,y)
cols = select.get_support(indices=True)
X_new = X.iloc[:,cols]
print(X_new)
# PART F: split data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2)
# PART G: train model with fit
 
regressor = LinearRegression()
trained_model_reg = regressor.fit(X_train, y_train) 
print("finished running with no bugs")
# PART H: test your model on 

#y_pred = trained_model_reg.predict(X_test)

r2_score = regressor.score(X_test,y_test)
print("r2_score=", r2_score)
print(r2_score*100,'%')
y_pred = regressor.predict(X_test)
print("r2_score=", sklearn.metrics.r2_score(y_test, y_pred))
print(r2_score*100,'%')

# plt.scatter(X_train, y_train, color='g') 
# plt.show()
# plt.plot(X_test, y_pred,color='k') 
#print(trained_model_reg.score(X_train, y_train))
#print(test_model_reg.score(X_test, y_train))



#
# do for linear regression:
# 
 

#lr = LinearRegreassion() 
#lr.fit(X_train, y_train)
#pred = lr.predict(X_test)



#
# do for classification:
#

# y = visits['HDRS_drop_level'] # level is one of 3 labels: ,'respone','non-response',and 'remission'
# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)
#knn = KNeighborsRegressor(n_neighbors=3) 3 for response, non- response and remission
#knn.fit(X_train, y_train)
#pred = knn.predict(X_test)
#mae = mean_absolute_error(y_test, pred) , to compute the mean absolute error (optional)


#BIOMARKERS:
#* Asymmetry alpha power (pair_mean_power_abs) fronal lobe
#* Asymmetry beta power (pair_mean_power_abs) central lobe
#* ...

""" 
IMBALANCED DATA SET --- WE DO THIS ONLY IF OUR ML ISN'T RELIABLE
Below is an example for undersampling - a technique to help the ML
be more reliable when the data isn't ideally distributed,
when "data" is the name of the final df we're working on:

shuffled = data.sample(frac=1,random_state=4)

# Put all the "smaller group" (which is the group that takes up the majority of the data) in a separate dataset.
smaller_group = shuffled.loc[shuffled['Class'] == 1]

#Randomly select n observations from the majority class
bigger_group = shuffled.loc[shuffled['column'] == 0].sample(n=n,random_state=42)

# Concatenate both dataframes again
normalized = pd.concat([smaller_group, bigger_group])

#plot the dataset after the undersampling, this part might not be the same for us
plt.figure(figsize=(8, 8))
sns.countplot('Class', data=normalized)
plt.title('Balanced Classes')
plt.show()

"""