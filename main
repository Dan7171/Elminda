from itertools import groupby
from pathlib import Path
from statistics import linear_regression
from time import strftime
import sklearn
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
import pandas as pd
import numpy as np
import datetime as dt
import warnings


warnings.simplefilter("ignore", UserWarning)
#import the_module_that_warns


def reformat_dates(df): 
    df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)
    df['just_date'] = pd.to_datetime(df['date']).dt.date
    newdate = dt.date(1998, 11, 8)
    df['just_date'].replace({pd.NaT: newdate}, inplace=True)
    df['just_date'] = df['just_date'].apply(lambda d: d.strftime("%d/%m/%Y"))
    addcol = df.pop('just_date')
    df.insert(2, 'just_date', addcol)
 

def unite_same_day_visits_to_the_first(df):
    """ for a subject with more than one EEG recording in the same day, keep only the first one"""
    df = df.sort_values(by=['subject','date'])
    df = df.reset_index()
    df = df.drop_duplicates(subset=['subject', 'just_date'], keep='first') #removes rows(visits) who has same subject with the same date, but leaves the first appearance
    df = df.reset_index()


def final_date_merge(df1, df2): 
    """merging df1 and df2 by subject and creates a final_date column for each visit """
    t = 2 #t is the valid maximun difference in days
    df = df1.merge(df2, how = 'inner',on = ['subject'])

    df['final_date'] =[d1 if time_difference(d1,d2)<= t else "remove" for d1,d2 in zip(df['just_date_x'],df['just_date_y'])] 
    df = df[df['final_date']!= 'remove']
    df = df.reset_index()
    df = df.drop_duplicates(subset=['final_date','subject'], keep='first')
    df = df.reset_index()
    return df 
 
def time_difference(d1_str, d2_str): #v
    d1 = dt.datetime.strptime(d1_str, "%d/%m/%Y")
    d2 = dt.datetime.strptime(d2_str, "%d/%m/%Y")
    delta = d2-d1
    difference = abs(delta.days)
    return difference   

def save_df_to_csv(df):
    path = Path()
    df.to_csv('visits.csv',index = False)


def get_percantage_change(df, column_name):
    pass
     

def group_by(df,c): #visits.groupby(['subject']), #df.groupby([c])
    """ equivalent to df.groupby([c]), I wrote this func for exampling the outputs!

    given a df (visits) returns all of the sub dfs, where each sub df belongs so one subjects:
    sub44
        visit  final_date
    46      1  22/08/2016
    47      2  29/08/2016
    48      3  09/05/2016
    49      5  20/09/2016
    50      6  27/09/2016
    sub83
        visit  final_date  HDRS-17
    53      1  12/11/2016     23.0
    54      2  18/12/2016     22.0
    55      3  25/12/2016     21.0
    56      4  01/01/2017     21.0
    57      5  01/08/2017     20.0
    58      6  16/01/2017     14.0
    .
    .
    ."""
    return df.groupby([c])
    
 

def get_group_by_name_from_all_groups(groups_by_coulumn,desired_group_name):
    """first getting all athe groups by 'subject'picks particulary the group of the subject passed in subject_id
    example: input = (groups_by_coulumn=visits.groupby['subject] (all groups by subjects),desired_group_name='sub83') -> output = :
    
    output = sub83,
    sub83
        visit  final_date  HDRS-17 .....
    53      1  12/11/2016     23.0
    54      2  18/12/2016     22.0
    55      3  25/12/2016     21.0
    56      4  01/01/2017     21.0
    57      5  01/08/2017     20.0
    58      6  16/01/2017     14.0"""
    
    for name, group in groups_by_coulumn: #name is group name('subject'), group is all its df
        if name == desired_group_name:
            return group # a df
            #print(name)
            #print(group[['visit','final_date']]) #see output example in ducumentation

def get_subject_first_visit_val_in_column_c(subject_id,d,c,):
    group = d[subject_id] # O(1)
    #group.sort_values(by='visit')
    return group.iloc[0][c]
    
def get_subject_last_visit_val_in_column_c(subject_id,d,c):
    group = d[subject_id] # O(1)
    #group.sort_values(by='visit')
    return group.iloc[len(group)-1][c]

def print_interesting_values_by_subject_grouping(df,d):
    """adding desired priniting columns which are unneccessary in model (like first_visit_HDRS-17),
    printing frouping by subject of all wanted featured and then deleting the added unnccessary ones"""
    add_HDRS_first_last_columns(visits,d) #adding 'first_visit_HDRS-17','last_visit_HDRS-17'  columns to visits
    #after addign unnccessary columns: print groups by subject:
    for name,group in group_by(df,'subject'):
        print(name)
        print(group[['visit','final_date','HDRS-17']])
        print(group[['first_visit_HDRS-17','last_visit_HDRS-17','change_HDRS-17']])
    #remove unneccessary
    df.drop(columns=['first_visit_HDRS-17','last_visit_HDRS-17'])



def get_subjects_with_increase_in_HDRS_or_with_zero_start_HDRS(visits):
    """returning all the strange subjects that has HDRS-17 == 0 in the
    first visit or had a positive percantage change in change_HDRS-17 (increased depression)"""
    
    strange_subjects = set(visits.iloc[i]['subject'] for i in range(len(visits)) if (visits.iloc[i]['change_HDRS-17'] == None or float(visits.iloc[i]['change_HDRS-17']) > 0))
    return strange_subjects
    
def get_subject_change_rate_in_column_c(subject,d,c):
    """Given a subject (value in the column 'subject') and a dictionary of key= 'subject' val = this 'subject' df,
    returns the value of the change rate from before to after, in the column named c,
    where the before change is the value of c in the first visit of this subject and
    after change is the value of c in the last visit of this subject."""
    subject_df = d[subject]
    before_change_val = subject_df.iloc[0][c] #the value of the 
    after_change_val = subject_df.iloc[len(subject_df)-1][c]
    if before_change_val != 0:
        percantage_change = ((after_change_val - before_change_val) / before_change_val) * 100
    else:
        percantage_change =  None
    return percantage_change

def add_HDRS_first_last_columns(df,d):
    """adding the mentioned colums columns which er are interested at about HDRS-17 to the df"""
    df['first_visit_HDRS-17'] = df['subject'].apply(lambda subject: get_subject_first_visit_val_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))
    df['last_visit_HDRS-17'] = df['subject'].apply(lambda subject: get_subject_last_visit_val_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))


def select_features_by_univariate_selection(df): 
    "by chi square, anova test, or corellation coefficient(pearson or spirman)?"
    "we will use the method selectKbest- which finds the K best features in terms of maximun corellation with the y variant"    
    pass
def seletct_features_by_feature_importance(df): #all 3 technices are usefull for small data sets only. not usefull for us.
    "part of the wraper methods"
    "USEFUL ONLY IN SMALL DATA SETS"
    "by wraper method. Some of them with no statistical tasks, but only one of 3 mechanismes:"
    "forward selection/backward elimination/recursion feature elimination. forward selection- start with no features, adding only highly corellated with y features to the model.  backward elimination: start with all of features, find the least significant feature on y, if its p val >0.05 drop it else leave it,"
    "and move on to the next feature untill end of features"
    pass
def select_features_by_correlation_matrix_heat_map(df):
    pass
#*** MAIN PART : ***

bna = pd.read_csv('BNA_data.csv')
clinical = pd.read_csv("Clinical_data_hebrew.csv") #Hebrew is reversed in the dataframe
reformat_dates(bna)
reformat_dates(clinical)
unite_same_day_visits_to_the_first(bna)  
visits = final_date_merge(bna, clinical) 
save_df_to_csv(visits)


visits_grouped_by_subject = group_by(visits,'subject') # all groups by subject, same as df.groupby(['subject])
subject_to_subject_group = {}
for name,group in visits_grouped_by_subject: # map all groups by key= name= subject, val = group = subject's df (for time complexity reasons)
    group.sort_values(by='visit')
    subject_to_subject_group[name] = group
visits['change_HDRS-17'] = visits['subject'].apply(lambda subject: get_subject_change_rate_in_column_c(subject,d=subject_to_subject_group,c='HDRS-17'))



#***READ:***
# all code above this line works properly. we have a new column of the improvement percantage in
# HDRS-17 named 'change_HDRS-17'.

# print the lines below to see what are the results  by now:

#just printing some interesting values for each subject:
print("INTERESTING VALUES FOR EACH SUBJECT")
print_interesting_values_by_subject_grouping(visits,subject_to_subject_group)
#subjects without improvement or with no first visit value of HDRS-17:
print("STRANGE SUBJECTS")
strange_subjects = set(visits.iloc[i]['subject'] for i in range(len(visits)) if (visits.iloc[i]['change_HDRS-17'] == "None" or float(visits.iloc[i]['change_HDRS-17']) > 0)) 
print(strange_subjects)


# ***READ:***
#From this point the bug starts. my goal is to remove all colums with string/date values (to leave only numbers because we need numbers only for the
# models training inside each coulumn). I try somehow to drop all the values which are strings and dates with the function get_dummies (maybe it's not what it supposed to do actually) but
#everytime I do that, it is adding the combination of 'final_date' X one of it's dates as a new column (around 1200 more columns)
#print the columns to see example. it also happens after I do visits_no_dates = visits.drop(['final_dates'])
#and printing its columns. same problem of adding columns

print("visits columns \n" , visits.columns) 
visits_dummies = pd.get_dummies(visits) #problem: get_dummies adding redudnat colums (example: 'final_date_29/8/2017 as a column. run (print(visit_model.coulums()) to see) looks like it combing final date with it's values))
visits_no_dummies = visits.drop(columns = visits_dummies)
print("visits without dummies \n" , visits.columns) 
print(visits_no_dummies.columns)
#maybe will help: https://stackoverflow.com/questions/50176096/removing-redundant-columns-when-using-get-dummies

#print(visits_model.columns)
# visits_model = pd.get_dummies(visits_model)
# print(visits_model.columns)


#***READ:***
#after debuging, the following lines should work properly

#X = visits_model.drop(columns= ['change_HDRS-17'])
#y = visits_model['change_HDRS-17']
#X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)

#best_features = SelectKBest(score_func=chi2,k=10)
#fit = best_features.fit(X,y) #when running fit, probllems caused by get_dummies start to effect





#
# do for linear regression:
# 
 

#lr = LinearRegreassion() 
#lr.fit(X_train, y_train)
#pred = lr.predict(X_test)



#
# do for classification:
#

# y = visits['HDRS_drop_level'] # level is one of 3 labels: ,'respone','non-response',and 'remission'
# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)
#knn = KNeighborsRegressor(n_neighbors=3) 3 for response, non- response and remission
#knn.fit(X_train, y_train)
#pred = knn.predict(X_test)
#mae = mean_absolute_error(y_test, pred) , to compute the mean absolute error (optional)


#BIOMARKERS:
#* Asymmetry alpha power (pair_mean_power_abs) fronal lobe
#* Asymmetry beta power (pair_mean_power_abs) central lobe
#* ...